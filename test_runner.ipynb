{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25590e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "logger = logging.getLogger(__name__)\n",
    "from src.core.factory import RetrieverFactory\n",
    "from src.core.base import CodeExample\n",
    "from src.retrievers.dense.database import CodeExampleDatabase\n",
    "\n",
    "# Configuration\n",
    "TRAINING_DATA_PATH = \"data/database/high-resource/method2test/reformat_test.jsonl\"\n",
    "BENCHMARK_REPO = \"Tessera2025/Tessera2025\"\n",
    "OUTPUT_DIR = \"data/constructed_prompt\"\n",
    "DATABASE_SAVE_PATH = \"data/database_index.pkl\"\n",
    "EMBEDDER_NAME=\"unixcoder\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc2b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=datasets.load_dataset(BENCHMARK_REPO,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b269ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    rust: Dataset({\n",
       "        features: ['function_component', 'function_name', 'focal_code', 'file_path', 'file_content', 'wrap_class', 'class_signature', 'struct_class', 'package_name'],\n",
       "        num_rows: 374\n",
       "    })\n",
       "    go: Dataset({\n",
       "        features: ['function_component', 'function_name', 'focal_code', 'file_path', 'file_content', 'wrap_class', 'class_signature', 'struct_class', 'package_name'],\n",
       "        num_rows: 372\n",
       "    })\n",
       "    julia: Dataset({\n",
       "        features: ['function_component', 'function_name', 'focal_code', 'file_path', 'file_content', 'wrap_class', 'class_signature', 'struct_class', 'package_name'],\n",
       "        num_rows: 417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0350dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _register_builtin_implementations():\n",
    "    \"\"\"Register built-in retriever implementations.\"\"\"\n",
    "    try:\n",
    "        from src.retrievers.dense.embedder import UniXcoderEmbedder\n",
    "        RetrieverFactory.register_embedder(EMBEDDER_NAME, UniXcoderEmbedder)\n",
    "    except ImportError:\n",
    "        logger.warning(\"Could not register UniXcoderEmbedder\")\n",
    "    \n",
    "    try:\n",
    "        from src.retrievers.dense.database import CodeExampleDatabase\n",
    "        RetrieverFactory.register_database(\"dense_vector\", CodeExampleDatabase)\n",
    "    except ImportError:\n",
    "        logger.warning(\"Could not register CodeExampleDatabase\")\n",
    "    \n",
    "    try:\n",
    "        from src.retrievers.fewshot_pipeline import FewShotTestGenerationPipeline\n",
    "        RetrieverFactory.register_pipeline(\"few_shot\", FewShotTestGenerationPipeline)\n",
    "    except ImportError:\n",
    "        logger.warning(\"Could not register FewShotTestGenerationPipeline\")\n",
    "\n",
    "\n",
    "# Register on module import\n",
    "_register_builtin_implementations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e016b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(file_path: str, max_examples: int = None) -> list:\n",
    "    \"\"\"\n",
    "    Load training data from JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSONL file\n",
    "        max_examples: Maximum number of examples to load (None = load all)\n",
    "        \n",
    "    Returns:\n",
    "        List of CodeExample objects\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    print(f\"Loading training data from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_examples and i >= max_examples:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    \n",
    "                    # Extract focal_method and unit_test\n",
    "                    # Adjust field names based on your JSONL structure\n",
    "                    focal_method = data.get('focal_method') \n",
    "                    unit_test = data.get('unit_test') \n",
    "                    \n",
    "                    if focal_method and unit_test:\n",
    "                        example = CodeExample(\n",
    "                            focal_method=focal_method,\n",
    "                            unit_test=unit_test,\n",
    "                            metadata=data.get('metadata', {})\n",
    "                        )\n",
    "                        examples.append(example)\n",
    "                    \n",
    "                    # Progress indicator\n",
    "                    if (i + 1) % 1000 == 0:\n",
    "                        print(f\"  Loaded {i + 1} examples...\")\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"  Warning: Skipping invalid JSON at line {i + 1}\")\n",
    "                    continue\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File not found: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"✓ Loaded {len(examples)} training examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def load_benchmark(repo_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Load benchmark test cases from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to benchmark JSON file\n",
    "        \n",
    "    Returns:\n",
    "        List of benchmark dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        dataset = datasets.load_dataset(repo_path)\n",
    "        benchmark_rust=dataset[\"rust\"].to_list()\n",
    "        benchmark_go=dataset[\"go\"].to_list()\n",
    "        benchmark_julia=dataset[\"julia\"].to_list()\n",
    "\n",
    "        \n",
    "        return benchmark_rust, benchmark_go, benchmark_julia\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File not found: {repo_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"  Error: Invalid JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "def to_jsonable(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return [to_jsonable(o) for o in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_jsonable(v) for k, v in obj.items()}\n",
    "    if hasattr(obj, \"to_dict\"):\n",
    "        return obj.to_dict()\n",
    "    return obj\n",
    "\n",
    "def save_benchmark(benchmark, output_path: str):\n",
    "    \"\"\"\n",
    "    Save a generated prompt to file.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt text\n",
    "        output_path: Path to save the prompt\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    jsonable_benchmark = [to_jsonable(d) for d in benchmark]\n",
    "\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in jsonable_benchmark:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90116ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedders': ['unixcoder'],\n",
       " 'databases': ['dense_vector'],\n",
       " 'pipelines': ['few_shot']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=RetrieverFactory()\n",
    "x.list_available_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c1b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RetrieverFactory.create_full_pipeline(\n",
    "    method=EMBEDDER_NAME,\n",
    "    db_type=\"dense_vector\",\n",
    "    pipeline_type=\"few_shot\",\n",
    "    pipeline_kwargs={\n",
    "        \"top_k\": 5,\n",
    "        \"similarity_threshold\": 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0787c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: data/database/high-resource/method2test/reformat_test.jsonl\n",
      "  Loaded 1000 examples...\n",
      "✓ Loaded 1000 training examples\n",
      "\n",
      "Step 3: Building retrieval database...\n",
      "--------------------------------------------------------------------------------\n",
      "Adding 1000 examples to database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 671196.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database built with 1000 examples\n",
      "\n",
      "Step 4: Saving database index...\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Database saved to: data/database_index.pkl\n",
      "\n",
      "Step 5: Loading benchmark...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "training_examples = load_training_data(\n",
    "    TRAINING_DATA_PATH,\n",
    "    max_examples= 1000 # Change to None to load all\n",
    ")\n",
    "\n",
    "if not training_examples:\n",
    "    print(\"⚠ No training data loaded. Please check the file path and format.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 3: Build database index\n",
    "print(\"Step 3: Building retrieval database...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"Adding {len(training_examples)} examples to database...\")\n",
    "pipeline.database.add_examples_bulk(training_examples)\n",
    "\n",
    "print(\"Building index (this may take a few minutes)...\")\n",
    "pipeline.database.build_index()\n",
    "\n",
    "print(f\"✓ Database built with {pipeline.database.size} examples\")\n",
    "print()\n",
    "\n",
    "# Step 4: Save database index\n",
    "print(\"Step 4: Saving database index...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "os.makedirs(os.path.dirname(DATABASE_SAVE_PATH), exist_ok=True)\n",
    "pipeline.database.save_index(DATABASE_SAVE_PATH)\n",
    "\n",
    "print(f\"✓ Database saved to: {DATABASE_SAVE_PATH}\")\n",
    "print()\n",
    "\n",
    "# Step 5: Load benchmark\n",
    "print(\"Step 5: Loading benchmark...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2622b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_rust,benchmark_go,benchmark_julia = load_benchmark(BENCHMARK_REPO)\n",
    "    \n",
    "if not (benchmark_go and benchmark_rust and benchmark_julia):\n",
    "    print(\"⚠ No benchmark cases loaded. Please check the file path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "193e7260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Generating prompts for benchmark cases...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Generate prompts for each benchmark case\n",
    "print(\"Step 6: Generating prompts for benchmark cases...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "prompts = []\n",
    "for lang in ['rust','go','julia']:\n",
    "    if lang=='rust':\n",
    "        benchmark_cases=benchmark_rust\n",
    "    elif lang=='go':\n",
    "        benchmark_cases=benchmark_go\n",
    "    else:\n",
    "        benchmark_cases=benchmark_julia\n",
    "    output_path = os.path.join(OUTPUT_DIR, EMBEDDER_NAME, lang,\"data_with_fewshot.jsonl\")\n",
    "    results=[]\n",
    "    for i, case in enumerate(benchmark_cases, 1):\n",
    "        case_id = case.get('id', f'case_{i}')\n",
    "        focal_method = case.get('focal_code')\n",
    "        \n",
    "        if not focal_method:\n",
    "            print(f\"  ⚠ Skipping case {case_id}: no focal_method found\")\n",
    "            continue\n",
    "            \n",
    "        # Generate prompt using pipeline\n",
    "        prompt = pipeline.run(focal_method)\n",
    "\n",
    "        result={\"id\":case_id,\"retrieved_context\":prompt}\n",
    "        results.append(result)\n",
    "    # Save prompt to file\n",
    "    save_benchmark(results, output_path)\n",
    "\n",
    "# print(f\"      ✓ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bade50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=to_jsonable(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3be0f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = z[\"retrieved_context\"][\"results\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c083469f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'focal_method': 'public boolean check(TsData data) {\\n\\n        clear();\\n        if (!testSeries(data)) {\\n            return false;\\n        }\\n        ModellingContext context = new ModellingContext();\\n        model_ = preprocessor_.process(data.drop(0, nback_), context);\\n        if (model_ == null) {\\n            return false;\\n        }\\n\\n        info_ = context.information;\\n        TsVariableList vars = model_.description.buildRegressionVariables();\\n        TsDomain fdomain = new TsDomain(model_.description.getSeriesDomain().getEnd(), nback_);\\n        List<DataBlock> x = vars.all().data(fdomain);\\n\\n        forecasts_ = new Forecasts();\\n\\n        RegArimaEstimation<SarimaModel> estimation\\n                = new RegArimaEstimation<>(model_.estimation.getRegArima(), model_.estimation.getLikelihood());\\n\\n        try {\\n            forecasts_.calcForecast(estimation, x, nback_, model_.description.getArimaComponent().getFreeParametersCount());\\n        } catch (RuntimeException err) {\\n            return false;\\n        }\\n\\n        y_ = data.fittoDomain(fdomain);\\n        fy_ = y_.clone();\\n        ofcasts_ = new TsData(fdomain);\\n        for (int i = 0; i < ofcasts_.getLength(); ++i) {\\n            ofcasts_.set(i, forecasts_.forecast(i));\\n        }\\n        model_.backTransform(ofcasts_, true, true);\\n\\n        List<ITsDataTransformation> transformations = model_.description.transformations();\\n        for (ITsDataTransformation tr : transformations) {\\n            tr.transform(fy_, null);\\n        }\\n\\n        return true;\\n    }',\n",
       " 'unit_test': '@Test\\n    public void testSO() {\\n        TramoSpecification spec = TramoSpecification.TRfull.clone();\\n        spec.getOutliers().add(OutlierType.SO);\\n        spec.getOutliers().setCriticalValue(3);\\n        CheckLast cl=new CheckLast(spec.build());\\n        assertTrue(cl.check(Data.P));\\n    }',\n",
       " 'metadata': {}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"example\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33b2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_examples = z[\"retrieved_prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63d0f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public boolean check(TsData data) {\n",
      "\n",
      "        clear();\n",
      "        if (!testSeries(data)) {\n",
      "            return false;\n",
      "        }\n",
      "        ModellingContext context = new ModellingContext();\n",
      "        model_ = preprocessor_.process(data.drop(0, nback_), context);\n",
      "        if (model_ == null) {\n",
      "            return false;\n",
      "        }\n",
      "\n",
      "        info_ = context.information;\n",
      "        TsVariableList vars = model_.description.buildRegressionVariables();\n",
      "        TsDomain fdomain = new TsDomain(model_.description.getSeriesDomain().getEnd(), nback_);\n",
      "        List<DataBlock> x = vars.all().data(fdomain);\n",
      "\n",
      "        forecasts_ = new Forecasts();\n",
      "\n",
      "        RegArimaEstimation<SarimaModel> estimation\n",
      "                = new RegArimaEstimation<>(model_.estimation.getRegArima(), model_.estimation.getLikelihood());\n",
      "\n",
      "        try {\n",
      "            forecasts_.calcForecast(estimation, x, nback_, model_.description.getArimaComponent().getFreeParametersCount());\n",
      "        } catch (RuntimeException err) {\n",
      "            return false;\n",
      "        }\n",
      "\n",
      "        y_ = data.fittoDomain(fdomain);\n",
      "        fy_ = y_.clone();\n",
      "        ofcasts_ = new TsData(fdomain);\n",
      "        for (int i = 0; i < ofcasts_.getLength(); ++i) {\n",
      "            ofcasts_.set(i, forecasts_.forecast(i));\n",
      "        }\n",
      "        model_.backTransform(ofcasts_, true, true);\n",
      "\n",
      "        List<ITsDataTransformation> transformations = model_.description.transformations();\n",
      "        for (ITsDataTransformation tr : transformations) {\n",
      "            tr.transform(fy_, null);\n",
      "        }\n",
      "\n",
      "        return true;\n",
      "    }\n",
      "@Test\n",
      "    public void testSO() {\n",
      "        TramoSpecification spec = TramoSpecification.TRfull.clone();\n",
      "        spec.getOutliers().add(OutlierType.SO);\n",
      "        spec.getOutliers().setCriticalValue(3);\n",
      "        CheckLast cl=new CheckLast(spec.build());\n",
      "        assertTrue(cl.check(Data.P));\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "print(item[\"example\"][\"focal_method\"]+\"\\n\"+item[\"example\"][\"unit_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933486c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
